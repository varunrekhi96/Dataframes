{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a733fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    " builder. \\\n",
    " config('spark.ui.port', '0'). \\\n",
    " config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    " enableHiveSupport(). \\\n",
    " master('yarn'). \\\n",
    " getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54d3d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:40801\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f05b1538390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85919466",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b967d2",
   "metadata": {},
   "source": [
    "#### Create a DataFrame using the sample data and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd8cae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Spring\",12.3),\n",
    "(\"Summer\",10.5),\n",
    "(\"Autumn\",8.2),\n",
    "(\"Winter\",15.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1343e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,schema=[\"season\",\"windspeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59464f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "554bd2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|season|windspeed|\n",
      "+------+---------+\n",
      "|Spring|     12.3|\n",
      "|Summer|     10.5|\n",
      "|Autumn|      8.2|\n",
      "|Winter|     15.1|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d8d25",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa59bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fab29b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField(\"library_name\", StringType()),\n",
    "StructField(\"location\", StringType()),\n",
    "StructField(\"books\", ArrayType(\n",
    "StructType([\n",
    "StructField(\"book_id\", StringType()),\n",
    "StructField(\"book_name\", StringType()),\n",
    "StructField(\"author\", StringType()),\n",
    "StructField(\"copies_available\", IntegerType())\n",
    "])\n",
    ")),\n",
    "StructField(\"members\", ArrayType(\n",
    "StructType([\n",
    "StructField(\"member_id\", StringType()),\n",
    "StructField(\"member_name\", StringType()),\n",
    "StructField(\"age\", IntegerType()),\n",
    "StructField(\"books_borrowed\", ArrayType(StringType()))\n",
    "])\n",
    "))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d5b9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "library_df = spark.read.schema(schema).json(\"/public/trendytech/datasets/library_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5ee9c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- library_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- books: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- book_id: string (nullable = true)\n",
      " |    |    |-- book_name: string (nullable = true)\n",
      " |    |    |-- author: string (nullable = true)\n",
      " |    |    |-- copies_available: integer (nullable = true)\n",
      " |-- members: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- member_id: string (nullable = true)\n",
      " |    |    |-- member_name: string (nullable = true)\n",
      " |    |    |-- age: integer (nullable = true)\n",
      " |    |    |-- books_borrowed: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "174b2c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+--------------------+--------------------+\n",
      "|     library_name|   location|               books|             members|\n",
      "+-----------------+-----------+--------------------+--------------------+\n",
      "|  Central Library|City Center|[[B001, The Great...|[[M001, John Smit...|\n",
      "|Community Library|     Suburb|[[B003, 1984, Geo...|[[M003, Michael B...|\n",
      "+-----------------+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d04b0b",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acc84fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load(\"/public/trendytech/datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b07bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- train_number: integer (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- seats_available: integer (nullable = true)\n",
      " |-- passenger_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- ticket_number: string (nullable = true)\n",
      " |-- seat_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0fab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|train_number|train_name|seats_available|passenger_name|age|ticket_number|seat_number|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "|         123|   Express|            100|          John| 25|         T123|         A1|\n",
      "|         123|   Express|            100|          Emma| 30|         T124|         B2|\n",
      "|         456| Superfast|            150|       Michael| 35|         T125|         C3|\n",
      "|         456| Superfast|            150|        Sophia| 40|         T126|         D4|\n",
      "|         789|     Local|             50|       William| 28|         T127|         E5|\n",
      "|         789|     Local|             50|        Sophia| 32|         T128|         F6|\n",
      "|         789|     Local|             50|        Oliver| 45|         T129|         G7|\n",
      "+------------+----------+---------------+--------------+---+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a28b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df = train_df.drop(\"passenger_name\",\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32bbb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+---------------+-------------+-----------+\n",
      "|train_number|train_name|seats_available|ticket_number|seat_number|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "|         123|   Express|            100|         T123|         A1|\n",
      "|         123|   Express|            100|         T124|         B2|\n",
      "|         456| Superfast|            150|         T125|         C3|\n",
      "|         456| Superfast|            150|         T126|         D4|\n",
      "|         789|     Local|             50|         T127|         E5|\n",
      "|         789|     Local|             50|         T128|         F6|\n",
      "|         789|     Local|             50|         T129|         G7|\n",
      "+------------+----------+---------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c15e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = dropped_df.dropDuplicates([\"train_number\", \"ticket_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28d3221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = drop_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7f81552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing duplicates: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows after removing duplicates:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a6b53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique train names: 3\n"
     ]
    }
   ],
   "source": [
    "distinct_departments = df.select(\"train_name\").distinct()\n",
    "num_departments = distinct_departments.count()\n",
    "print(\"Number of unique train names:\", num_departments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a043c22",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "012d6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records read: 22\n"
     ]
    }
   ],
   "source": [
    "schema=\"store_id integer,product string,quantity integer,revenue double\"\n",
    "df_permissive =spark.read.schema(schema).option(\"mode\",\"permissive\").json(\"/public/trendytech/datasets/sales_data.json\")\n",
    "num_records_permissive = df_permissive.count()\n",
    "print(\"Number of records read:\", num_records_permissive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "085bf7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-------+\n",
      "|store_id|   product|quantity|revenue|\n",
      "+--------+----------+--------+-------+\n",
      "|       1|     Apple|      10|  100.0|\n",
      "|       2|    Banana|      15|   75.0|\n",
      "|       3|    Orange|      12|   90.0|\n",
      "|       4|     Mango|       8|  120.0|\n",
      "|       5|     Grape|      20|  150.0|\n",
      "|       6|Watermelon|       5|   50.0|\n",
      "|       7|Strawberry|      18|  108.0|\n",
      "|       8| Pineapple|      14|  140.0|\n",
      "|       9|    Cherry|       7|  105.0|\n",
      "|      10|      Pear|       9|   81.0|\n",
      "|      11| Blueberry|      11|   88.0|\n",
      "|      12|      Kiwi|      16|  128.0|\n",
      "|      13|     Peach|      13|   91.0|\n",
      "|      14|      Plum|       6|   54.0|\n",
      "|      15|     Lemon|      10|   70.0|\n",
      "|      16| Raspberry|      17|  136.0|\n",
      "|      17|   Coconut|       4|   80.0|\n",
      "|      18|   Avocado|      11|   99.0|\n",
      "|      19|Blackberry|       8|   64.0|\n",
      "+--------+----------+--------+-------+\n",
      "\n",
      "Number of records read: 21\n",
      "Number of dropped malformed records: 1\n"
     ]
    }
   ],
   "source": [
    "df_dropmalformed = spark.read.option(\"mode\",\"dropmalformed\").schema(schema).json(\"/public/trendytech/datasets/sales_data.json\")\n",
    "df_dropmalformed.show()\n",
    "num_records_dropmalformed = df_dropmalformed.count()\n",
    "num_corrupt_records_dropmalformed = num_records_permissive - num_records_dropmalformed\n",
    "print(\"Number of records read:\", num_records_dropmalformed)\n",
    "print(\"Number of dropped malformed records:\",num_corrupt_records_dropmalformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failfast =spark.read.option(\"mode\",\"failfast\").schema(schema).json(\"/user/itv005357/sales_data.json\")\n",
    "df_failfast.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c777c2",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9ed8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"patient_id integer,admission_date date,discharge_date date,diagnosis string,doctor_id integer,total_cost float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19c90055",
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_df=spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".schema(schema) \\\n",
    ".option(\"dateFormat\",\"MM-dd-yyyy\") \\\n",
    ".load(\"/public/trendytech/datasets/hospital.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd83f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|doctor_id|total_cost|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|      101|    5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|      102|    7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|      103|    3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      104|   15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|      105|    2500.0|\n",
      "+----------+--------------+--------------+-------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hosp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8fd5ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+----------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|total_cost|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|    5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|    7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|    3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|   15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|    2500.0|\n",
      "+----------+--------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_df = hosp_df.drop(\"doctor_id\")\n",
    "hospital_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "225ed810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|\n",
      "+----------+--------------+--------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_new_df = hospital_df.withColumnRenamed(\"total_cost\",\"hospital_bill\")\n",
    "hospital_new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "172fdc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a4c3f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_expr_df = hospital_new_df.withColumn(\"duration_of_stay\",expr(\"datediff(discharge_date, admission_date)\"))\n",
    "hospital_expr_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b2acce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_price_df = hospital_expr_df.withColumn(\"adjusted_total_cost\",expr(\"CASE WHEN diagnosis LIKE 'Heart Attack' THEN hospital_bill * 1.5 WHEN diagnosis LIKE 'Appendicitis' THEN hospital_bill * 1.2 ELSE hospital_bill END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6268597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|patient_id|admission_date|discharge_date|    diagnosis|hospital_bill|duration_of_stay|adjusted_total_cost|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "|         1|    2022-01-01|    2022-01-10|    Pneumonia|       5000.0|               9|             5000.0|\n",
      "|         2|    2022-02-05|    2022-02-09| Appendicitis|       7000.0|               4|             8400.0|\n",
      "|         3|    2022-03-12|    2022-03-18|Fractured Arm|       3500.0|               6|             3500.0|\n",
      "|         4|    2022-04-02|    2022-04-08| Heart Attack|      15000.0|               6|            22500.0|\n",
      "|         5|    2022-05-05|    2022-05-07|    Influenza|       2500.0|               2|             2500.0|\n",
      "+----------+--------------+--------------+-------------+-------------+----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_price_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ef106ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------------+-------------------+\n",
      "|patient_id|    diagnosis|hospital_bill|adjusted_total_cost|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "|         1|    Pneumonia|       5000.0|             5000.0|\n",
      "|         2| Appendicitis|       7000.0|             8400.0|\n",
      "|         3|Fractured Arm|       3500.0|             3500.0|\n",
      "|         4| Heart Attack|      15000.0|            22500.0|\n",
      "|         5|    Influenza|       2500.0|             2500.0|\n",
      "+----------+-------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hospital_final_df = hospital_price_df.select(\"patient_id\", \"diagnosis\",\"hospital_bill\", \"adjusted_total_cost\")\n",
    "hospital_final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700a547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
